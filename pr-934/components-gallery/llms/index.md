---
hide:
  - toc
  - navigation
---
# LLMs Gallery



<div class="grid cards" markdown>


-   :simple-openai:{ .lg .middle } __OpenAILLM__

    ---

    OpenAI LLM implementation running the async API client.

    [:octicons-arrow-right-24: OpenAILLM](openaillm.md){ .bottom }

-   :material-brain:{ .lg .middle } __ClientvLLM__

    ---

    A client for the `vLLM` server implementing the OpenAI API specification.

    [:octicons-arrow-right-24: ClientvLLM](clientvllm.md){ .bottom }

-   :material-brain:{ .lg .middle } __AnyscaleLLM__

    ---

    Anyscale LLM implementation running the async API client of OpenAI.

    [:octicons-arrow-right-24: AnyscaleLLM](anyscalellm.md){ .bottom }

-   :material-microsoft-azure:{ .lg .middle } __AzureOpenAILLM__

    ---

    Azure OpenAI LLM implementation running the async API client.

    [:octicons-arrow-right-24: AzureOpenAILLM](azureopenaillm.md){ .bottom }

-   :material-brain:{ .lg .middle } __TogetherLLM__

    ---

    TogetherLLM LLM implementation running the async API client of OpenAI.

    [:octicons-arrow-right-24: TogetherLLM](togetherllm.md){ .bottom }

-   :material-brain:{ .lg .middle } __AnthropicLLM__

    ---

    Anthropic LLM implementation running the Async API client.

    [:octicons-arrow-right-24: AnthropicLLM](anthropicllm.md){ .bottom }

-   :material-brain:{ .lg .middle } __CohereLLM__

    ---

    Cohere API implementation using the async client for concurrent text generation.

    [:octicons-arrow-right-24: CohereLLM](coherellm.md){ .bottom }

-   :material-brain:{ .lg .middle } __GroqLLM__

    ---

    Groq API implementation using the async client for concurrent text generation.

    [:octicons-arrow-right-24: GroqLLM](groqllm.md){ .bottom }

-   :hugging:{ .lg .middle } __InferenceEndpointsLLM__

    ---

    InferenceEndpoints LLM implementation running the async API client.

    [:octicons-arrow-right-24: InferenceEndpointsLLM](inferenceendpointsllm.md){ .bottom }

-   :material-brain:{ .lg .middle } __LiteLLM__

    ---

    LiteLLM implementation running the async API client.

    [:octicons-arrow-right-24: LiteLLM](litellm.md){ .bottom }

-   :material-brain:{ .lg .middle } __MistralLLM__

    ---

    Mistral LLM implementation running the async API client.

    [:octicons-arrow-right-24: MistralLLM](mistralllm.md){ .bottom }

-   :material-brain:{ .lg .middle } __MixtureOfAgentsLLM__

    ---

    `Mixture-of-Agents` implementation.

    [:octicons-arrow-right-24: MixtureOfAgentsLLM](mixtureofagentsllm.md){ .bottom }

-   :material-brain:{ .lg .middle } __OllamaLLM__

    ---

    Ollama LLM implementation running the Async API client.

    [:octicons-arrow-right-24: OllamaLLM](ollamallm.md){ .bottom }

-   :simple-googlecloud:{ .lg .middle } __VertexAILLM__

    ---

    VertexAI LLM implementation running the async API clients for Gemini.

    [:octicons-arrow-right-24: VertexAILLM](vertexaillm.md){ .bottom }

-   :material-brain:{ .lg .middle } __vLLM__

    ---

    `vLLM` library LLM implementation.

    [:octicons-arrow-right-24: vLLM](vllm.md){ .bottom }

-   :hugging:{ .lg .middle } __TransformersLLM__

    ---

    Hugging Face `transformers` library LLM implementation using the text generation

    [:octicons-arrow-right-24: TransformersLLM](transformersllm.md){ .bottom }

-   :material-brain:{ .lg .middle } __LlamaCppLLM__

    ---

    llama.cpp LLM implementation running the Python bindings for the C++ code.

    [:octicons-arrow-right-24: LlamaCppLLM](llamacppllm.md){ .bottom }


</div>