{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a preference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Goal**: Generate a synthetic preference dataset for DPO/ORPO.\n",
    "- **Libraries**: [argilla](https://github.com/argilla-io/argilla), [hf-inference-endpoints](https://github.com/huggingface/huggingface_hub)\n",
    "- **Components**: [LoadDataFromHub](https://distilabel.argilla.io/latest/components-gallery/steps/loaddatafromhub/), [TextGeneration](https://distilabel.argilla.io/latest/components-gallery/tasks/textgeneration/), [UltraFeedback](https://distilabel.argilla.io/latest/components-gallery/tasks/ultrafeedback/), [GroupColumns](https://distilabel.argilla.io/latest/components-gallery/steps/groupcolumns/), [FormatTextGenerationDPO](https://distilabel.argilla.io/latest/components-gallery/steps/formattextgenerationdpo/), [PreferenceToArgilla](https://distilabel.argilla.io/latest/components-gallery/steps/textgenerationtoargilla/), [InferenceEndpointsLLM](https://distilabel.argilla.io/latest/components-gallery/llms/inferenceendpointsllm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the dependencies\n",
    "\n",
    "To complete this tutorial, you need to install the distilabel SDK and a few third-party libraries via pip. We will be using **the free but rate-limited Hugging Face serverless Inference API** for this tutorial, so we need to install this as an extra distilabel dependency. You can install them by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"distilabel[hf-inference-endpoints]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers>=4.0,<5.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distilabel.llms import InferenceEndpointsLLM\n",
    "from distilabel.pipeline import Pipeline\n",
    "from distilabel.steps import (\n",
    "    LoadDataFromHub,\n",
    "    GroupColumns,\n",
    "    FormatTextGenerationDPO,\n",
    "    PreferenceToArgilla,\n",
    ")\n",
    "from distilabel.steps.tasks import TextGeneration, UltraFeedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need an `HF_TOKEN` to use the HF Inference Endpoints. Log in to use it directly within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"), add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (optional) Deploy Argilla\n",
    "\n",
    "You can skip this step or replace it with any other data evaluation tool, but the quality of your model will suffer from a lack of data quality, so we do recommend looking at your data. If you already have deployed Argilla, you can skip this step. Otherwise, you can quickly deploy Argilla following [this guide](https://docs.argilla.io/latest/getting_started/quickstart/). \n",
    "\n",
    "Along with that, you will need to install argilla as distilabel extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"distilabel[argilla, hf-inference-endpoints]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate our preference dataset, we will need to define a `Pipeline` with all the necessary steps. Below, we will go over each step in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "We will use as source data the `argilla/10Kprompts-mini` dataset from the [Hugging Face Hub](https://huggingface.co/datasets/argilla/10Kprompts-mini).\n",
    "\n",
    "- Component: `LoadDataFromHub`\n",
    "- Input columns: `instruction` and `topic`, the same as in the loaded dataset\n",
    "- Output columns: `instruction` and `topic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset = LoadDataFromHub(\n",
    "        repo_id= \"argilla/10Kprompts-mini\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate responses\n",
    "\n",
    "We need to generate the responses for the given instructions. We will use two different models available in the Hugging Face Hub through the Serverless Inference API: [`meta-llama/Meta-Llama-3-8B-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) and [`mistralai/Mixtral-8x7B-Instruct-v0.1`](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1). We will also indicate the generation parameters for each model.\n",
    "\n",
    "- Component: `TextGeneration` task with LLMs using `InferenceEndpointsLLM`\n",
    "- Input columns: `instruction`\n",
    "- Output columns: `generation`, `distilabel_metadata`, `model_name` for each model\n",
    "\n",
    "For your use case and to improve the results, you can use any [other LLM of your choice](https://distilabel.argilla.io/latest/components-gallery/llms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_responses = [\n",
    "    TextGeneration(\n",
    "        llm=InferenceEndpointsLLM(\n",
    "            model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            tokenizer_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "        )\n",
    "    ),\n",
    "    TextGeneration(\n",
    "        llm=InferenceEndpointsLLM(\n",
    "            model_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            tokenizer_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "        )\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group the responses\n",
    "\n",
    "The task to evaluate the responses needs as input a list of generations. However, each model response was saved in the generation column of the subsets `text_generation_0` and `text_generation_1`. We will combine these two columns into a single column and the `default` subset.\n",
    "\n",
    "- Component: `GroupColumns`\n",
    "- Input columns: `generation` and `model_name`from `text_generation_0` and `text_generation_1`\n",
    "- Output columns: `generations` and `model_names`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_responses = GroupColumns(\n",
    "        columns=[\"generation\", \"model_name\"],\n",
    "        output_columns=[\"generations\", \"model_names\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the responses\n",
    "\n",
    "To build our preference dataset, we need to evaluate the responses generated by the models. We will use [`meta-llama/Meta-Llama-3-70B-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) for this, applying the `UltraFeedback` task that judges the responses according to different dimensions (helpfulness, honesty, instruction-following, truthfulness).\n",
    "\n",
    "- Component: `UltraFeedback` task with LLMs using `InferenceEndpointsLLM`\n",
    "- Input columns: `generations`\n",
    "- Output columns: `ratings`, `rationales`, `distilabel_metadata`, `model_name`\n",
    "\n",
    "For your use case and to improve the results, you can use any [other LLM of your choice](https://distilabel.argilla.io/latest/components-gallery/llms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses = UltraFeedback(\n",
    "    aspect=\"overall-rating\",\n",
    "    llm=InferenceEndpointsLLM(\n",
    "        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to a preference dataset\n",
    "\n",
    "- You can automatically convert it to a preference dataset with the `chosen` and `rejected` columns.\n",
    "    - Component: `FormatTextGenerationDPO` step\n",
    "    - Input columns: `instruction`, `generations`, `generation_models`, `ratings`\n",
    "    - Output columns: `prompt`, `prompt_id`, `chosen`, `chosen_model`, `chosen_rating`, `rejected`, `rejected_model`, `rejected_rating`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_dpo = FormatTextGenerationDPO()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or you can use Argilla to manually label the data and convert it to a preference dataset.\n",
    "    - Component: `PreferenceToArgilla` step\n",
    "    - Input columns: `instruction`, `generations`, `generation_models`, `ratings`\n",
    "    - Output columns: `instruction`, `generations`, `generation_models`, `ratings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "to_argilla = PreferenceToArgilla(\n",
    "    dataset_name=\"preference-dataset\",\n",
    "    dataset_workspace=\"argilla\",\n",
    "    api_url=\"https://[your-owner-name]-[your_space_name].hf.space\",\n",
    "    api_key=\"[your-api-key]\",\n",
    "    num_generations=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can see the full pipeline definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pipeline(name=\"generate-dataset\") as pipeline:\n",
    "\n",
    "    load_dataset = LoadDataFromHub(repo_id=\"argilla/10Kprompts-mini\")\n",
    "\n",
    "    generate_responses = [\n",
    "        TextGeneration(\n",
    "            llm=InferenceEndpointsLLM(\n",
    "                model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                tokenizer_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "            )\n",
    "        ),\n",
    "        TextGeneration(\n",
    "            llm=InferenceEndpointsLLM(\n",
    "                model_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                tokenizer_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "                generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "            )\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    group_responses = GroupColumns(\n",
    "        columns=[\"generation\", \"model_name\"],\n",
    "        output_columns=[\"generations\", \"model_names\"],\n",
    "    )\n",
    "\n",
    "    evaluate_responses = UltraFeedback(\n",
    "        aspect=\"overall-rating\",\n",
    "        llm=InferenceEndpointsLLM(\n",
    "            model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "            tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "            generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    format_dpo = FormatTextGenerationDPO()\n",
    "    \n",
    "    to_argilla = PreferenceToArgilla(\n",
    "        dataset_name=\"preference-dataset\",\n",
    "        dataset_workspace=\"argilla\",\n",
    "        api_url=\"https://[your-owner-name]-[your_space_name].hf.space\",\n",
    "        api_key=\"[your-api-key]\",\n",
    "        num_generations=2\n",
    "    )\n",
    "\n",
    "    load_dataset >> generate_responses >> group_responses >> evaluate_responses >> [format_dpo, to_argilla]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the pipeline and generate the preference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the preference dataset! If you have loaded the data to Argilla, you can [start annotating in the Argilla UI](https://docs.argilla.io/latest/how_to_guides/annotate/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can push the dataset to the Hub for sharing with the community and [embed it to explore the data](https://huggingface.co/docs/hub/datasets-viewer-embed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset.push_to_hub(\"[your-owner-name]/example-preference-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "  src=\"https://huggingface.co/datasets/distilabel-internal-testing/example-generate-preference-dataset/embed/viewer/format_text_generation_d_p_o_0/train\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"560px\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we showcased the detailed steps to build a pipeline for generating a preference dataset using distilabel. You can customize this pipeline for your own use cases and share your datasets with the community through the Hugging Face Hub, or use them to train a model for DPO or ORPO.\n",
    "\n",
    "We used a dataset containing prompts to generate responses using two different models through the serverless Hugging Face Inference API. Next, we evaluated the responses using a third model, following the UltraFeedback standards. Finally, we converted the data to a preference dataset and used Argilla for further curation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilabel-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
