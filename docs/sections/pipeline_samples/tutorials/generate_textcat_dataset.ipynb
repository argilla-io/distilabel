{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic text classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Goal**: Generate synthetic text classification data to augment an imbalanced and limited dataset for training a topic classifier. In addition, generate new data for training a fact-based versus opinion-based classifier to add a new label.\n",
    "- **Libraries**: [argilla](https://github.com/argilla-io/argilla), [hf-inference-endpoints](https://github.com/huggingface/huggingface_hub), [SetFit](https://github.com/huggingface/setfit)\n",
    "- **Components**: [LoadDataFromDicts](https://distilabel.argilla.io/latest/components-gallery/steps/loaddatafromdicts/), [EmbeddingTaskGenerator](https://distilabel.argilla.io/latest/components-gallery/tasks/embeddingtaskgenerator/), [GenerateTextClassificationData](https://distilabel.argilla.io/latest/components-gallery/tasks/generatetextclassificationdata/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the dependencies\n",
    "\n",
    "To complete this tutorial, you need to install the distilabel SDK and a few third-party libraries via pip. We will be using **the free but rate-limited Hugging Face serverless Inference API** for this tutorial, so we need to install this as an extra distilabel dependency. You can install them by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"distilabel[hf-inference-endpoints]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers~=4.40\" \"torch~=2.0\" \"setfit~=1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note \"Troubleshooting\"\n",
    "    The SetFit library still relies on `huggingface_hub==0.23.5`. However, distilabel requires the latest version, `huggingface_hub>=0.24`. To avoid any conflicts, we recommend installing from source: `pip install git+https://github.com/Wauplin/setfit@dont-use-deprecated-dataset-filter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from distilabel.llms import InferenceEndpointsLLM\n",
    "from distilabel.pipeline import Pipeline\n",
    "from distilabel.steps import LoadDataFromDicts\n",
    "from distilabel.steps.tasks import (\n",
    "    EmbeddingTaskGenerator,\n",
    "    GenerateTextClassificationData,\n",
    ")\n",
    "from setfit import SetFitModel, Trainer, sample_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need an `HF_TOKEN` to use the HF Inference Endpoints. Log in to use it directly within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"), add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (optional) Deploy Argilla\n",
    "\n",
    "You can skip this step or replace it with any other data evaluation tool, but the quality of your model will suffer from a lack of data quality, so we do recommend looking at your data. If you already deployed Argilla, you can skip this step. Otherwise, you can quickly deploy Argilla following [this guide](https://docs.argilla.io/latest/getting_started/quickstart/). \n",
    "\n",
    "Along with that, you will need to install Argilla as a distilabel extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"distilabel[argilla, hf-inference-endpoints]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [`fancyzhx/ag_news`](https://huggingface.co/datasets/fancyzhx/ag_news) dataset from the Hugging Face Hub as our original data source. To simulate a real-world scenario with imbalanced and limited data, we will load only 20 samples from this dataset.\n",
    "\n",
    "<iframe\n",
    "  src=\"https://huggingface.co/datasets/fancyzhx/ag_news/embed/viewer/default/train\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"560px\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset = load_dataset(\"fancyzhx/ag_news\", split=\"train[-20:]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can retrieve the available labels in the dataset and examine the current data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_topic = hf_dataset.features[\"label\"].names\n",
    "id2str = {i: labels_topic[i] for i in range(len(labels_topic))}\n",
    "id2str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 12, 1: 6, 2: 2})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(hf_dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, the dataset is imbalanced, with most samples falling under the `World` category, while the `Sci/Tech` category is entirely missing. Moreover, there are insufficient samples to effectively train a topic classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the text classification task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GenerateTextClassificationData` task will be in charged of generating the synthetic text classification data for the given tasks. We can generate text classification tasks using the `EmbeddingTaskGenerator` component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying customer sentiment in e-commerce product reviews as positive or negative.\n",
      "Identifying spam or non-spam emails in email inboxes.\n",
      "Determining the topic of a news article as politics, sports, or entertainment.\n",
      "Categorizing medical reports as urgent or non-urgent.\n",
      "Classifying text messages as promotional or transactional.\n",
      "Identifying the language of a text as English, Spanish, or French.\n",
      "Classifying social media posts as hate speech or not hate speech.\n",
      "Detecting the intent behind a search query as informational or transactional.\n",
      "Categorizing job descriptions as part-time or full-time.\n",
      "Classifying text as formal or informal.\n",
      "Identifying the tone of a text as sarcastic or sincere.\n",
      "Determining the subject of a text as business, education, or entertainment.\n",
      "Classifying phone calls as sales or support.\n",
      "Identifying the sentiment of a text as positive, negative, or neutral.\n",
      "Classifying reviews as 1-5 stars based on text content.\n",
      "Classifying text as machine generated or human generated.\n",
      "Determining the purpose of a text as persuasive or informative.\n",
      "Classifying text as abstract or concrete.\n",
      "Categorizing text as technical or non-technical.\n",
      "Identifying the genre of a text as fiction or non-fiction.\n",
      "Classifying text as subjective or objective.\n",
      "Classifying text as noisy or clean.\n"
     ]
    }
   ],
   "source": [
    "tasks = EmbeddingTaskGenerator(\n",
    "    category=\"text-classification\",\n",
    "    flatten_tasks=True,\n",
    "    llm=InferenceEndpointsLLM(\n",
    "        model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        tokenizer_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "        generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "    ),\n",
    ")\n",
    "tasks.load()\n",
    "result = next(tasks.process())\n",
    "\n",
    "for i in range(len(result[0])):\n",
    "    print(result[0][i][\"task\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our use case, we only need to generate data for two tasks: a topic classification task and a fact versus opinion classification task. Therefore, we will define the tasks accordingly. As we have enough samples for the `World` category, we will omit it from our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tasks = [\n",
    "    {\n",
    "        \"task\": \"Determining the topic of a news article as sports, technology, or business\"\n",
    "    },\n",
    "    {\"task\": \"Classifying news article as fact-based or opinion-based\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to define and run the pipeline. As mentioned, we will load the written tasks and feed them into the `GenerateTextClassificationData` task. To ensure the diversity and quality of the data, we will use a different model via the `InferenceEndpointsLLM`, with degrees of clarity and generation arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pipeline(\"texcat-generation-pipeline\") as pipeline:\n",
    "\n",
    "    tasks_generator = LoadDataFromDicts(data=classification_tasks)\n",
    "\n",
    "    generate_data = [\n",
    "        GenerateTextClassificationData(\n",
    "            language=\"English\",\n",
    "            difficulty=\"college\",\n",
    "            clarity=\"clear\",\n",
    "            num_generations=20,\n",
    "            llm=InferenceEndpointsLLM(\n",
    "                model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                tokenizer_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "                generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.4},\n",
    "            ),\n",
    "            input_batch_size=5,\n",
    "        ),\n",
    "        GenerateTextClassificationData(\n",
    "            language=\"English\",\n",
    "            difficulty=\"college\",\n",
    "            clarity=\"understandable with some effort\",\n",
    "            num_generations=20,\n",
    "            llm=InferenceEndpointsLLM(\n",
    "                model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                tokenizer_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                generation_kwargs={\n",
    "                    \"max_new_tokens\": 512,\n",
    "                    \"temperature\": 0.8,\n",
    "                    \"top_p\": 0.95,\n",
    "                },\n",
    "            ),\n",
    "            input_batch_size=5,\n",
    "        ),\n",
    "        GenerateTextClassificationData(\n",
    "            language=\"English\",\n",
    "            difficulty=\"college\",\n",
    "            clarity=\"ambiguous\",\n",
    "            num_generations=20,\n",
    "            llm=InferenceEndpointsLLM(\n",
    "                model_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                tokenizer_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "                generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "            ),\n",
    "            input_batch_size=2,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for task in generate_data:\n",
    "        tasks_generator.connect(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the pipeline and generate the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can push the dataset to the Hub for sharing with the community and [embed it to explore the data](https://huggingface.co/docs/hub/datasets-viewer-embed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset.push_to_hub(\"[your-owner-name]/example-texcat-generation-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "  src=\"https://huggingface.co/datasets/distilabel-internal-testing/example-texcat-generation-dataset/embed/viewer/generate_text_classification_data_1/train\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"560px\"\n",
    "></iframe>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the distiset distribution, we can confirm that it includes at least the 8 required samples for each label to train our classification models with SetFit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'fact-based': 40,\n",
       "         None: 29,\n",
       "         'sports': 17,\n",
       "         'technology': 14,\n",
       "         'business': 11,\n",
       "         'opinion-based': 9})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = [\n",
    "    entry[\"label\"]\n",
    "    for dataset_name in distiset\n",
    "    for entry in distiset[dataset_name][\"train\"]\n",
    "]\n",
    "\n",
    "Counter(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Argilla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note \"Get started in Argilla\"\n",
    "    If you are not familiar with Argilla, we recommend taking a look at the [Argilla quickstart docs](https://docs.argilla.io/latest/getting_started/quickstart/). Alternatively, you can use your Hugging Face account to login to the [Argilla demo Space](https://argilla-argilla-template-space.hf.space).\n",
    "\n",
    "To get the most out of our data, we will use Argilla. First, we need to connect to the Argilla instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argilla as rg\n",
    "\n",
    "# Replace api_url with your url if using Docker\n",
    "# Replace api_key with your API key under \"My Settings\" in the UI\n",
    "# Uncomment the last line and set your HF_TOKEN if your space is private\n",
    "client = rg.Argilla(\n",
    "    api_url=\"https://[your-owner-name]-[your_space_name].hf.space\",\n",
    "    api_key=\"[your-api-key]\",\n",
    "    # headers={\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a `Dataset` for each task, with an input `TextField` for the text classification text and a `LabelQuestion` to ensure the generated labels are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_texcat_dataset(dataset_name, labels):\n",
    "    settings = rg.Settings(\n",
    "        fields=[rg.TextField(\"text\")],\n",
    "        questions=[\n",
    "            rg.LabelQuestion(\n",
    "                name=\"label\",\n",
    "                title=\"Classify the texts according to the following labels\",\n",
    "                labels=labels,\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    return rg.Dataset(name=dataset_name, settings=settings).create()\n",
    "\n",
    "\n",
    "dataset_topic = create_texcat_dataset(\"topic-classification2\", labels_topic)\n",
    "\n",
    "labels_fact_opinion = [\"Fact-based\", \"Opinion-based\"]\n",
    "dataset_fact_opinion = create_texcat_dataset(\n",
    "    \"fact-opinion-classification2\", labels_fact_opinion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can upload the generated data to Argilla and evaluate it. We will use the generated labels as suggestions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_topic = []\n",
    "records_fact_opinion = []\n",
    "\n",
    "\n",
    "def create_record(entry):\n",
    "    return rg.Record(\n",
    "        fields={\"text\": entry[\"input_text\"]},\n",
    "        suggestions=[\n",
    "            rg.Suggestion(\n",
    "                \"label\",\n",
    "                value=(\n",
    "                    \"Sci/Tech\"\n",
    "                    if entry[\"label\"].lower() == \"technology\"\n",
    "                    else entry[\"label\"].capitalize()\n",
    "                ),\n",
    "                agent=entry[\"model_name\"],\n",
    "                type=\"model\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "for dataset_name in distiset.keys():\n",
    "    for entry in distiset[dataset_name].get(\"train\", []):\n",
    "        if entry[\"label\"] is None:\n",
    "            continue\n",
    "        if entry[\"task\"] == classification_tasks[0][\"task\"]:\n",
    "            records_topic.append(create_record(entry))\n",
    "        elif entry[\"task\"] == classification_tasks[1][\"task\"]:\n",
    "            records_fact_opinion.append(create_record(entry))\n",
    "\n",
    "\n",
    "dataset_topic.records.log(records_topic)\n",
    "dataset_fact_opinion.records.log(records_fact_opinion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start the annotation process. Just open the dataset in the Argilla UI and start annotating the records. If the suggestions are correct, you can just click on `Submit`. Otherwise, you can select the correct label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note\n",
    "    Check this [how-to guide](https://docs.argilla.io/latest/how_to_guides/annotate/) to know more about annotating in the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the annotation, we will have two robust datasets to train the corresponding models. In our case, we will fine-tune using SetFit. However, you can select the one that best fits your requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by retrieving the annotated data from Argilla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_topic = client.datasets(\"topic-classification2\")\n",
    "dataset_fact_opinion = client.datasets(\"fact-opinion-classification2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_filter = rg.Query(filter=rg.Filter((\"response.status\", \"==\", \"submitted\")))\n",
    "\n",
    "submitted_topic = dataset_topic.records(status_filter).to_list(flatten=True)\n",
    "submitted_fact_opinion = dataset_fact_opinion.records(status_filter).to_list(\n",
    "    flatten=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to format the data to be compatible with SetFit. In the case of the topic classification, we will need to combine the synthetic data with the original data. Moreover, we will add an `id` column to track the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the original dataset for topic classification\n",
    "train_records_topic = hf_dataset.to_list()\n",
    "\n",
    "for i, record in enumerate(train_records_topic):\n",
    "    record[\"label\"] = id2str[record[\"label\"]]\n",
    "    record[\"id\"] = i\n",
    "\n",
    "# Add the synthetic data to the original dataset\n",
    "num = len(train_records_topic)\n",
    "\n",
    "train_records_topic.extend(\n",
    "    [\n",
    "        {\n",
    "            \"text\": r[\"text\"],\n",
    "            \"label\": r[\"label.responses\"][0],\n",
    "            \"id\": num + i,\n",
    "        }\n",
    "        for i, r in enumerate(submitted_topic)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the data for the fact-opinion classification\n",
    "train_records_fact_opinion = [\n",
    "    {\n",
    "        \"text\": r[\"text\"],\n",
    "        \"label\": r[\"label.responses\"][0],\n",
    "        \"id\": i,\n",
    "    }\n",
    "    for i, r in enumerate(submitted_fact_opinion)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the data distribution now, we can see that we have enough samples for each label to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Sports': 23, 'Sci/Tech': 14, 'Business': 13, 'World': 12})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [record[\"label\"] for record in train_records_topic]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Fact-based': 40, 'Opinion-based': 9})"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [record[\"label\"] for record in train_records_fact_opinion]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create our training and validation datasets. The training dataset will gather 8 samples by label. In this case, the validation datasets will contain the remaining samples not included in the training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_split(dataset, label_column, num_samples):\n",
    "    train_dataset = sample_dataset(\n",
    "        dataset, label_column=label_column, num_samples=num_samples\n",
    "    )\n",
    "    eval_dataset = dataset.filter(lambda x: x[\"id\"] not in set(train_dataset[\"id\"]))\n",
    "    return train_dataset, eval_dataset\n",
    "\n",
    "\n",
    "dataset_topic_full = Dataset.from_list(train_records_topic)\n",
    "dataset_fact_opinion_full = Dataset.from_list(train_records_fact_opinion)\n",
    "\n",
    "train_dataset_topic, eval_dataset_topic = sample_and_split(\n",
    "    dataset_topic_full, \"label\", 8\n",
    ")\n",
    "train_dataset_fact_opinion, eval_dataset_fact_opinion = sample_and_split(\n",
    "    dataset_fact_opinion_full, \"label\", 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our models for each task! We will use [TaylorAI/bge-micro-v2](https://huggingface.co/TaylorAI/bge-micro-v2), available in the Hugging Face Hub. You can check the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) to select the best model for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name, dataset, eval_dataset):\n",
    "    model = SetFitModel.from_pretrained(model_name)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate(eval_dataset)\n",
    "    print(metrics)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 768\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1873, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 4.9767, 'train_samples_per_second': 154.318, 'train_steps_per_second': 9.645, 'epoch': 1.0}\n",
      "{'accuracy': 0.8333333333333334}\n"
     ]
    }
   ],
   "source": [
    "model_topic = train_model(\n",
    "    model_name=\"TaylorAI/bge-micro-v2\",\n",
    "    dataset=train_dataset_topic,\n",
    "    eval_dataset=eval_dataset_topic,\n",
    ")\n",
    "model_topic.save_pretrained(\"topic_classification_model\")\n",
    "model_topic = SetFitModel.from_pretrained(\"topic_classification_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 144\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n",
      "  Total optimization steps = 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2985, 'learning_rate': 2e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 0.8327, 'train_samples_per_second': 172.931, 'train_steps_per_second': 10.808, 'epoch': 1.0}\n",
      "{'accuracy': 0.9090909090909091}\n"
     ]
    }
   ],
   "source": [
    "model_fact_opinion = train_model(\n",
    "    model_name=\"TaylorAI/bge-micro-v2\",\n",
    "    dataset=train_dataset_fact_opinion,\n",
    "    eval_dataset=eval_dataset_fact_opinion,\n",
    ")\n",
    "model_fact_opinion.save_pretrained(\"fact_opinion_classification_model\")\n",
    "model_fact_opinion = SetFitModel.from_pretrained(\"fact_opinion_classification_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà! The models are now trained and ready to be used. You can start making predictions to check the model's performance and add the new label. Optionally, you can continue using distilabel to generate additional data or Argilla to verify the quality of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input, labels):\n",
    "    model.labels = labels\n",
    "    prediction = model.predict([input])\n",
    "    return prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sci/Tech'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    model_topic, \"The new iPhone is expected to be released next month.\", labels_topic\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Opinion-based'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\n",
    "    model_fact_opinion,\n",
    "    \"The new iPhone is expected to be released next month.\",\n",
    "    labels_fact_opinion,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we showcased the detailed steps to build a pipeline for generating text classification data using distilabel. You can customize this pipeline for your own use cases and share your datasets with the community through the Hugging Face Hub.\n",
    "\n",
    "We defined two text classification tasks—a topic classification task and a fact versus opinion classification task—and generated new data using various models via the serverless Hugging Face Inference API. Then, we curated the generated data with Argilla. Finally, we trained the models with SetFit using both the original and synthetic data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilabel-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
