{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean an existing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Goal**: Clean an existing dataset by providing AI feedback on the quality of the data.\n",
    "- **Libraries**: [argilla](https://github.com/argilla-io/argilla), [hf-inference-endpoints](https://github.com/huggingface/huggingface_hub)\n",
    "- **Components**: [LoadDataFromDicts](https://distilabel.argilla.io/dev/components-gallery/steps/loaddatafromdicts/), [UltraFeedback](https://distilabel.argilla.io/latest/components-gallery/tasks/ultrafeedback/), [KeepColumns](https://distilabel.argilla.io/latest/components-gallery/steps/groupcolumns/), [PreferenceToArgilla](https://distilabel.argilla.io/latest/components-gallery/steps/textgenerationtoargilla/), [InferenceEndpointsLLM](https://distilabel.argilla.io/latest/components-gallery/llms/inferenceendpointsllm/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the dependencies\n",
    "\n",
    "To complete this tutorial, you need to install the distilabel SDK and a few third-party libraries via pip. We will be using **the free but rate-limited Hugging Face serverless Inference API** for this tutorial, so we need to install this as an extra distilabel dependency. You can install them by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"argilla[hf-inference-endpoints]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"transformers>=4.0,<5.0\" \"torch>=2.0,<3.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from distilabel.llms import InferenceEndpointsLLM\n",
    "from distilabel.pipeline import Pipeline\n",
    "from distilabel.steps import (\n",
    "    KeepColumns,\n",
    "    LoadDataFromDicts,\n",
    "    PreferenceToArgilla,\n",
    ")\n",
    "from distilabel.steps.tasks import UltraFeedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need an `HF_TOKEN` to use the HF Inference Endpoints. Login to use it directly within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"), add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### (optional) Deploy Argilla\n",
    "\n",
    "You can skip this step or replace it with any other data evaluation tool, but the quality of your model will suffer from a lack of data quality, so we do recommend looking at your data. If you already have deployed Argilla, you can skip this step. Otherwise, you can quickly deploy Argilla following [this guide](https://docs.argilla.io/latest/getting_started/quickstart/). \n",
    "\n",
    "Along with that, you will need to install argilla as distilabel extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"distilabel[argilla, hf-inference-endpoints]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we will clean a preference dataset, so we will use the `Intel/orca_dpo_pairs` dataset from the [Hugging Face Hub](https://huggingface.co/datasets/Intel/orca_dpo_pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Intel/orca_dpo_pairs\", split=\"train[:20]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will shuffle the `chosen` and `rejected` columns to avoid any bias in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_track(chosen, rejected):\n",
    "    pair = [chosen, rejected]\n",
    "    random.shuffle(pair)\n",
    "    order = [\"chosen\" if x == chosen else \"rejected\" for x in pair]\n",
    "    return {\"generations\": pair, \"order\": order}\n",
    "\n",
    "dataset = dataset.map(lambda x: shuffle_and_track(x[\"chosen\"], x[\"rejected\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "??? tip \"As a custom step\"\n",
    "    You can also [create a custom step](../../how_to_guides/basic/step/global_step.md) in a separate module, import it and add it to the pipeline after loading the `orca_dpo_pairs` dataset using the `LoadDataFromHub` step.\n",
    "\n",
    "    ```python title=\"shuffle_step.py\"\n",
    "    from typing import TYPE_CHECKING, List\n",
    "    from distilabel.steps import GlobalStep, StepInput\n",
    "\n",
    "    if TYPE_CHECKING:\n",
    "        from distilabel.steps.typing import StepOutput\n",
    "        \n",
    "    import random\n",
    "\n",
    "    class ShuffleStep(GlobalStep):\n",
    "        @property\n",
    "        def inputs(self) -> List[str]:\n",
    "            return [\"instruction\", \"chosen\", \"rejected\"]\n",
    "\n",
    "        @property\n",
    "        def outputs(self) -> List[str]:\n",
    "            return [\"instruction\", \"generations\", \"order\"]\n",
    "\n",
    "        def process(self, inputs: StepInput) -> \"StepOutput\":  # type: ignore\n",
    "            outputs = []\n",
    "\n",
    "            for input in inputs:\n",
    "                chosen = input[\"chosen\"]\n",
    "                rejected = input[\"rejected\"]\n",
    "                pair = [chosen, rejected]\n",
    "                random.shuffle(pair)\n",
    "                order = [\"chosen\" if x == chosen else \"rejected\" for x in pair]\n",
    "                \n",
    "                outputs.append({\"instruction\": input[\"instruction\"], \"generations\": pair, \"order\": order})\n",
    "\n",
    "            yield outputs\n",
    "    ```\n",
    "    \n",
    "    ```python\n",
    "    from shuffle_step import ShuffleStep\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean an existing preference dataset, we will need to define a `Pipeline` with all the necessary steps. However, a similar workflow can be used to clean a SFT dataset. Below, we will go over each step in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Load the dataset**: We will use the dataset we just shuffled as source data.\n",
    "    - Component: `LoadDataFromDicts`\n",
    "    - Input columns: `system`, `question`, `chosen`, `rejected`, `generations` and `order`, the same keys as in the loaded list of dictionaries.\n",
    "    - Output columns: `system`, `instruction`, `chosen`, `rejected`, `generations` and `order`. We will use `output_mappings` to rename the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset = LoadDataFromDicts(\n",
    "        data=dataset,\n",
    "        output_mappings={\"question\": \"instruction\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Evaluate the responses**: To evaluate the quality of the responses, we will use [`meta-llama/Meta-Llama-3-70B-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct), applying the `UltraFeedback` task that judges the responses according to different dimensions (helpfulness, honesty, instruction-following, truthfulness). For an SFT dataset, you can use [`PrometheusEval`](../papers/prometheus.md) instead.\n",
    "    - Component: `UltraFeedback` task with LLMs using `InferenceEndpointsLLM`\n",
    "    - Input columns: `generations`\n",
    "    - Output columns: `ratings`, `rationales`, `distilabel_metadata`, `model_name`\n",
    "\n",
    "    For your use case and to improve the results, you can use any [other LLM of your choice](https://distilabel.argilla.io/latest/components-gallery/llms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_responses = UltraFeedback(\n",
    "    aspect=\"overall-rating\",\n",
    "    llm=InferenceEndpointsLLM(\n",
    "        model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Keep only the required columns**: We will get rid of the unneeded columns.\n",
    "    - Component: `KeepColumns`\n",
    "    - Input columns: `system`, `instruction`, `chosen`, `rejected`, `generations`, `ratings`, `rationales`, `distilabel_metadata` and `model_name`\n",
    "    - Output columns: `instruction`, `chosen`, `rejected`, `generations` and `order`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = KeepColumns(\n",
    "    columns = [\"instruction\", \"generations\", \"order\", \"ratings\", \"rationales\", \"model_name\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(Optional) Further data curation**: You can use Argilla to further curate your data.\n",
    "    -  Component: `PreferenceToArgilla` step\n",
    "    - Input columns: `instruction`, `generations`, `generation_models`, `ratings`\n",
    "    - Output columns: `instruction`, `generations`, `generation_models`, `ratings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_argilla = PreferenceToArgilla(\n",
    "    dataset_name=\"cleaned-dataset\",\n",
    "    dataset_workspace=\"argilla\",\n",
    "    api_url=\"https://[your-owner-name]-[your_space_name].hf.space\",\n",
    "    api_key=\"[your-api-key]\",\n",
    "    num_generations=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can see the full pipeline definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distilabel.pipeline import Pipeline\n",
    "from distilabel.steps import LoadDataFromDicts\n",
    "from distilabel.steps.tasks import UltraFeedback\n",
    "\n",
    "with Pipeline(name=\"clean-dataset\") as pipeline:\n",
    "\n",
    "    load_dataset = LoadDataFromDicts(\n",
    "            data=dataset,\n",
    "            output_mappings={\"question\": \"instruction\"}\n",
    "        )\n",
    "\n",
    "    evaluate_responses = UltraFeedback(\n",
    "        aspect=\"overall-rating\",\n",
    "        llm=InferenceEndpointsLLM(\n",
    "            model_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "            tokenizer_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "            generation_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.7},\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    keep_columns = KeepColumns(\n",
    "        columns = [\"instruction\", \"generations\", \"order\", \"ratings\", \"rationales\", \"model_name\"]\n",
    "    )\n",
    "\n",
    "    to_argilla = PreferenceToArgilla(\n",
    "        dataset_name=\"cleaned-dataset\",\n",
    "        dataset_workspace=\"argilla\",\n",
    "        api_url=\"https://[your-owner-name]-[your_space_name].hf.space\",\n",
    "        api_key=\"[your-api-key]\",\n",
    "        num_generations=2\n",
    "    )\n",
    "    \n",
    "    load_dataset >> evaluate_responses >> [keep_columns, to_argilla]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the pipeline and clean our preference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it! If you have loaded the data to Argilla, you can [start annotating in the Argilla UI](https://docs.argilla.io/latest/how_to_guides/annotate/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can push the dataset to the Hub for sharing with the community and [embed it to explore the data](https://huggingface.co/docs/hub/datasets-viewer-embed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distiset.push_to_hub(\"[your-owner-name]/example-cleaned-preference-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<iframe\n",
    "  src=\"https://huggingface.co/datasets/sdiazlor/example-cleaned-preference-dataset/embed/viewer/keep_columns_0/train\"\n",
    "  frameborder=\"0\"\n",
    "  width=\"100%\"\n",
    "  height=\"560px\"\n",
    "></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we showcased the detailed steps to build a pipeline for cleaning a preference dataset using distilabel. However, you can customize this pipeline for your own use cases, such as cleaning an SFT dataset or adding custom steps.\n",
    "\n",
    "We used a preference dataset as our starting point and shuffled the data to avoid any bias. Next, we evaluated the responses using a model through the serverless Hugging Face Inference API, following the UltraFeedback standards. Finally, we kept the needed columns and used Argilla for further curation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distilabel-tutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
